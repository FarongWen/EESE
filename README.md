<div align="center">

    
 <div>
  <h1>The Ever-Evolving Science Exam</h1>

_What constitutes a good science benchmark?_


  <div>
      <a href="https://github.com/junyingwang959" target="_blank">Junying Wang</a><sup>*</sup>,
      <a href="https://zzc-1998.github.io/" target="_blank">Zicheng Zhang</a><sup>*</sup><sup>#</sup>,
      <a>Yijin Guo</a><sup>*</sup>,
      <a>Farong Wen</a><sup>*</sup>,
      <a>Ye Shen</a>,
      <a>Yingji Liang</a>,
  </div>

<div>
      <a>Yalun Wu</a>,
      <a>Wenzhe Li</a>,
      <a href="https://github.com/lcysyzxdxc" target="_blank">Chunyi Li</a>,
      <a href="https://zijianchen98.github.io/" target="_blank">Zijian Chen</a>,
      <a href="https://jiaqisjtu.github.io/" target="_blank">Qi Jia</a>,
      <a href="https://ee.sjtu.edu.cn/en/FacultyDetail.aspx?id=24&infoid=153&flag=153" target="_blank">Guangtao Zhai</a><sup>#</sup>,
  </div>
  <div>
  Shanghai Artificial Intelligence Laboratory
  </div>   
<div>
<sup>*</sup>Equal contribution. <sup>#</sup>Corresponding author. 
   </div>
    

<a href="https://github.com/aiben-ch/EESE"><strong>Github</strong></a>  |
<a href="https://aiben.ch/"><strong>Team Work</strong></a> 

  
    
 <div style="width: 80%; text-align: center; margin:auto;">
      <img style="width:80%" src="3R.png">
  </div>
</div>
 
</div>

## Release
- [2025/7/22]ðŸ”¥  Our paper is submmitted online.

## key Contribution
- **A large-scale, high-quality science benchmark pool**: We construct EESE-Pool, a 100K+ science question-answer pair pool across 5 disciplines and 500+ subfields, with diverse formats and rigorous quality control. We design three-stage Data Engine (Transcription, Expansion, and Categorization) and Data Refinement (a Parallel Three-Branch Refinement Framework) to ensure range, reach, and rigor.
- **A dynamic, leakage-resilient evaluation set**: We propose EESE, a 500-instance subset periodically updated (regular resampling 500 instances from the EESE-Pool), maintaining representativeness while reducing leakage risk and evaluation overhead.
- **Comprehensive evaluation of LLMs**: We evaluate 32 leading models (open- and closed-source) on EESE-Pool and EESE, revealing significant performance gaps across disciplines, the effectiveness of refinement in improving quality, and the trade-offs between inference cost and science ability. The findings offer insights for future science benchmarks.

<div></div>

<div style="width: 95%; text-align: center; margin:auto;">
      <img style="width:95%" src="performance_subplots_4.png">
  </div>
  


## Contact

Please contact any of the first authors of this paper for queries.

- Zicheng Zhang, `zhangzicheng@pjlab.org.cn`, @zzc-1998
- Junying Wang, `wangjunying@pjlab.org.cn`, @junyingwang959

